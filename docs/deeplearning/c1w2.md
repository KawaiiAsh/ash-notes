

Basics of Neural Network Programming
# 1- Logistic Regression as a Neural Network

## 1.1- Binary Classification
1. Example: 给一张64x64像素的图片图片，判断是否含有猫
2. 获取图片的RGB像素值
![Screen-Shot-2018-06-02-at-17.20.31](content/images/2018/06/Screen-Shot-2018-06-02-at-17.20.31.jpg)
3. 并unroll成一个vector \\(X^{(i)}\\)
![Screen-Shot-2018-06-02-at-17.20.42](content/images/2018/06/Screen-Shot-2018-06-02-at-17.20.42.jpg)
4. 所有的vector组成数据集矩阵\\(X\\)
![Screen-Shot-2018-06-02-at-17.25.21](content/images/2018/06/Screen-Shot-2018-06-02-at-17.25.21.jpg)
特别注意，\\(X\\)的行是\\(n\\)，列是\\(m\\)，和Machine learning中的定义正好是转置的关系。这样有个好处，每条测试集在矩阵中都是以列向量的形式存在。


5. DeepLearning常用Notations:
- \\(m\\): number of examples in datasets
- \\(n\_x\\): input size（即feature的个数）
- \\(n\_y\\): output size（即分类个数）
- \\(X \\in \mathbb{R}^{n\_x\times m} \\) ：the input matrix
- \\(Y \\in \mathbb{R}^{n\_y\times m} \\) ：the input matrix
- 带括号的上标\\(^{(i)}\\)，表示和training example相关的计数

完整的notation，可以参考课程中提供的PDF: *Standard notations for Deep Learning*
![Screen-Shot-2018-06-10-at-21.21.45](content/images/2018/06/Screen-Shot-2018-06-10-at-21.21.45.jpg)
![Screen-Shot-2018-06-10-at-21.21.58](content/images/2018/06/Screen-Shot-2018-06-10-at-21.21.58.jpg)


6. 使用Python中的reshape方法，整理矩阵的维度。

## 1.2- Logistic Regression
1. 问题描述：
Logsitic Regression要求输出y不是0就是1。The goal of logistic regression is to minimize the error between its predictions and training data.

![Screen-Shot-2018-06-02-at-17.37.35](content/images/2018/06/Screen-Shot-2018-06-02-at-17.37.35.jpg)
2. sigmoid function

![Screen-Shot-2018-06-02-at-17.45.47](content/images/2018/06/Screen-Shot-2018-06-02-at-17.45.47.jpg)

这里有个疑问，为什么sigmoid处理后的值，可以代表y=1概率？

参考：[Logistic distribution](https://stats.stackexchange.com/questions/80611/problem-understanding-the-logistic-regression-link-function/80623#80623)
2. 引入参数w, b，其实就是Machine learning中用的是θ，但DeepLearning中分别用w和b表示。其中w是vector，b是real number
4. 这里 𝑦̂ 就是Machine learning里面的hypothesis function: h(θ)



## 1.2- Logistic Regression Cost Funciton

1. Loss (error) function的定义：
![Screen-Shot-2018-06-02-at-17.52.42](content/images/2018/06/Screen-Shot-2018-06-02-at-17.52.42.jpg)
分成y为0和1两种情况去理解这个函数，**本质上就是对\\(\hat y\\)做对数处理**而已。
因为对数处理后确实**达到了Loss function的要求**（我自己的理解）：1.值域是大于等于0的实数集。 2. 随着𝑦̂ 单调递减。y=𝑦̂ 的时候为0，反之趋向于∞。3. 是参数的凸函数（convex）4. 是y和𝑦̂的函数 

2. 没有使用square error，因为是non-convex，无法使用Gradient Descent算法
3. Loss function是针对单个training example的，而**Cost function是Loss Function的在所有training example上的均值**。
![Screen-Shot-2018-06-02-at-18.03.20](content/images/2018/06/Screen-Shot-2018-06-02-at-18.03.20.jpg)
在Machine learning里，没有引入Loss Function，其实有一个Loss Function，更好理解。

## 1.3- Gradient Descent
Gradient Descent的原理（Intuition）：按梯度最大的方向逼近最小值。
![Screen-Shot-2018-06-02-at-20.48.07](content/images/2018/06/Screen-Shot-2018-06-02-at-20.48.07.jpg)

Gradient Descent算法步骤：
1. Initialize \\(w\\), \\(b\\) to zero
2. repeat：

$$ w\ :=w - w\frac{\partial J( w,b)}{\partial w}$$
$$ b\ :=b - b\frac{\partial J( w,b)}{\partial b}$$

## 1.4- Derivatives
为不了解导数的人介绍导数的直观含义，这里不作说明了。

## 1.5- More Derivative Examples
为不了解导数的人介绍导数的直观含义，这里不作说明了。

## 1.6- Computation graph
从左到右：计算函数J
从右到左：计算J对参数w和b导数

## 1.7- Derivative with a Computation Graph
* 其实是就是复合函数的链式法则。
* 计算图左边的变量的偏导数依赖于右边的偏导数，右边的偏导数计算后，可以被左边的计算复用。

在Python中表示偏导数
$$dvar = \frac{\partial J}{\partial var}$$

## 1.8- Logistic Regression Gradient Descent
使用Computation Graph计算

![Screen-Shot-2018-06-04-at-08.26.07](content/images/2018/06/Screen-Shot-2018-06-04-at-08.26.07.jpg)

虽然，测试集是离散的，但并不代表对w的导数是离散的，这两者没有任何关系。**始终注意：在gradient Descent的时候，x是常量**

## 1.9- Gradient Descent on m Examples
Cost Function的偏导是Loss Function偏导的均值：

$$\frac{\partial J(w,b)}{\partial w\_j} =\frac{1}{m} \sum\_{i=1}^{m} \frac{\partial \mathcal{L}(a^{(i)},y^{(i)})}{\partial w\_j^{(i)}}$$

Gradient Descent算法过程：
1. 求导过程
 求导过程又通常是先forward propagation求cost function，然后再backward propagation求到w和b的导数
3. 下降过程
使用到w和b的导数，迭代做梯度下降过程。

下面的截图就是一个非向量化的实现：
左边是求导过程，右边是梯度下降过程
![Screen-Shot-2018-06-04-at-09.02.27](content/images/2018/06/Screen-Shot-2018-06-04-at-09.02.27.jpg)


# 2- Python and Vectorization
## 2.1- Vectorization
1. 什么是Vectorization：将 for loop 尽可能转换为矩阵运算。举例：
$$z = w^Tx + b$$
![Screen-Shot-2018-06-04-at-09.13.25](content/images/2018/06/Screen-Shot-2018-06-04-at-09.13.25.jpg)

2. vectorization的好处：conciser code, but faster execution
一个简单的对比实验：1,000,000大小的两个向量内积计算，for loop要比Vectorization快300倍。
在DeepLearning时代，vectorization是一项重要的技能。

3. SIMD
Both CPU and GPU have parallelization instructions(i.e. SIMD, Signle Instruction Multiple Data)

## 2.2- More vectorization Examples
1. 原则：whenever possible, avoid explict for-loops
2. 使用Element wised的矩阵运算，将函数作用在每个矩阵元素上，比如：
    * np.exp()
    * np.log()
    * np.abs()
    * np.maxium()
    * 1/v
    * v\*\*2

## 2.3- Vectorizing logistic Regression

![Screen-Shot-2018-06-04-at-19.50.07](content/images/2018/06/Screen-Shot-2018-06-04-at-19.50.07.jpg)

$$A = \sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$$

（我始终觉得A应该小写，毕竟还是一个行向量）

个人经验：
1. 首先，熟悉每个变量的记号和维度，必要的话，可以画出来，更直观。
2. 先从一个样本做向量化，再把m个样本的操作向量化。
3. for-loop里面是循环乘法，则向量化一定是一个乘法形式，若对于不确定乘法的左右关系，是否需转置，可以根据目标变量的维度推测。或者先乘起来，再根据目标变量看是否要转置。

## 2.4- Vectorizing Logistic Regression's Gradient Output

推导过程

![Screen-Shot-2018-06-09-at-18.35.35](content/images/2018/06/Screen-Shot-2018-06-09-at-18.35.35.jpg)

最终向量化的形式是：

$$ \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T$$
$$ \frac{\partial J}{\partial b} = \frac{1}{m} \sum\_{i=1}^m (a^{(i)}-y^{(i)})$$

## 2.5- Broadcasting in Python
在matlab和Python中，都默认支持不同维度的变量做element-wised的计算（+-x/等）。所谓**Broadcasting**其实就是高纬度数组和低纬度数组计算时，将低维的变量**通过复制的方式向高维扩展维度**，再做运算。但要注意：
1. 低纬度数组与高纬度共有的维度，元素个数必须一样，比如一个shape是(5,3)的数组可以和一个shape是(5,)的数组相加，但不能和一个(4,)的数组相加。
2. 维度相同的数组的数组，是不能broadcasting的，比如一个shape是(5,3)的数组和一个shape是(5,2)的数组运算，后者是无法broadcasting的
3. 某个维度的个数是1，等同于这个维度不存在，可以broadcasting，比如shape是(5,3)和(5,1)的数组可以运算。

总的来说，就是broadcasting要做某个维度的复制，必须在赋值的时候行得通。比如shape是(5,3)和shape是(5,2)的数组运算，后者要将2复制为3是行不通的，因为存在两行，那取哪一行？

Andrew的一个经验：如果对某个数组的shape不确定，可以用reshape显式的调用一下，确保维度正确。

补充：numpy中，类似sum的函数，经常涉及axis参数，可以取值为0或1，甚至其他。经常记不住，这里我查了了一下，是这样的（[原文](https://stackoverflow.com/questions/17079279/how-is-axis-indexed-in-numpys-array)）：
1. **axis的数字，和数组的shape参数的索引是对应的**。比如一个数组的shape是(5,6)，则代表5个row，6个column。即在shape中，row和column的个数的索引是0和1。也就第1个坐标，在shape中的第一个元素，**索引是0，代表row的方向**；第2个坐标，在shape中的第2个元素，**索引是1，代表row的方向**。
2. 对于sum函数，axis指的是sum“**沿着**”的方向，经过计算，这个方向的维度因为求和后就消失了，比如sum(axis=0)代表是沿着“row”方向进行求和，
3. 当然axis可以是一个tupe，那就相当于沿着多个多个方向求和。
4. sum如果不传入axis参数，默认是对所有维度求和。

## 2.6- A note on python/numpy vectors
broadcasting的一个弱点：可能隐藏潜在的错误，比如一个计算中本来要去两个运算的数组维度一样，如果没有broadcasting，就会直接报错；而broadcasting允许可继续执行。

**rank 1 array问题**：shape是(x,)的数组，既不是行向量，也不是列向量，没法参与正常的矩阵运算，应该总是使用(x,1)或(1,x)的shape来表示向量。但可以通过reshape方法将rank 1 array转换为行向量或列向量。（什么是rank，就是一个数组的维度）

![Screen-Shot-2018-06-10-at-18.03.37](content/images/2018/06/Screen-Shot-2018-06-10-at-18.03.37.jpg)

## 2.8- Explanation of logistic regression cost function (optional)
计算结果𝑦̂代表了给定样本x，y=1的概率，即𝑦̂=P(y=1|x)

但为什么可以对应到概率呢？参考：为什么sigmoid可以代表概率，涉及Logistic distribution
https://stats.stackexchange.com/questions/80611/problem-understanding-the-logistic-regression-link-function/80623#80623

Loss function其实就是对概率P(y|x)取对数：

![Screen-Shot-2018-06-10-at-18.28.41](content/images/2018/06/Screen-Shot-2018-06-10-at-18.28.41.jpg)

Loss function越小，则取到和实际值y的概率越大。

所有样本的Cost function：

![Screen-Shot-2018-06-10-at-18.32.40](content/images/2018/06/Screen-Shot-2018-06-10-at-18.32.40.jpg)

# 3- Heros of Deep Learning：Pieter Abbeel interview

![Screen-Shot-2018-06-10-at-18.34.23](content/images/2018/06/Screen-Shot-2018-06-10-at-18.34.23.jpg)

1. Pieter Abbeel专注于deep reinforcement learning.

2. advice for people entrying AI
    * A good time to enter AI. High demand.
    * Not just read things or watch videos but **try things out**.
    * With frameworks like **TensorFlow, Chainer, Theano, PyTorch and so forth**, it's very easy to get going and get something up and running very quickly.

3. Andrew Ng: We live in good times. If people want to learn. 

4. what are the things that deep reinforcement learning is already working really well at?
    * learning to play games from pixels
    * robot inventing walking, running, standing up with a single algorithm. 





