# 激活函数的选择

在深度学习中，选择合适的激活函数是至关重要的。

## 常见激活函数

1. **Sigmoid函数**：
   $$\sigma(x) = \frac{1}{1 + e^{-x}}$$
    - 优点：输出范围在0到1之间，适合用于二分类问题的输出层。
    - 缺点：容易出现梯度消失问题，不适合深度网络。

2. **ReLU函数**（Rectified Linear Unit）：
   $$\text{ReLU}(x) = \max(0, x)$$
    - 优点：计算简单，解决了梯度消失问题，能够快速收敛。
    - 缺点：存在神经元死亡问题。

3. **Leaky ReLU函数**：
   $$\text{Leaky ReLU}(x) = \max(\alpha x, x), \text{ 其中 } \alpha \text{ 是小于1的参数}$$
    - 优点：解决了ReLU中的神经元死亡问题。
    - 缺点：可能会引入额外的超参数。

4. **ELU函数**（Exponential Linear Unit）：
   $$\text{ELU}(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha(e^x - 1) & \text{otherwise} \end{cases}$$
    - 优点：类似于Leaky ReLU，且保证所有输出都是负的。
    - 缺点：计算相对复杂。

5. **Tanh函数**：
   $$\text{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$
    - 优点：输出范围在-1到1之间，解决了Sigmoid函数的零中心问题。
    - 缺点：仍然存在梯度消失问题。

## 其它激活函数

除了常见的激活函数外，还有许多其他的选择，下面是一些常见的激活函数及其特点：

6. **Softmax函数**：
   $$\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j}^{n} e^{x_j}}$$
    - 通常用于多分类问题的输出层，将输出转换为概率分布。

7. **Swish函数**：
   $$\text{Swish}(x) = x \cdot \text{sigmoid}(x)$$
    - 由Google提出，结合了ReLU和sigmoid的优点。

8. **GELU函数**（Gaussian Error Linear Unit）：
   $$\text{GELU}(x) = x \cdot \Phi(x)$$
    - OpenAI提出，逼近高斯误差线性单元。

9. **Hard Swish函数**：
   $$\text{Hard Swish}(x) = x \cdot \max(0, \min(1, x + 3)) / 6$$
    - 华为提出，是一种ReLU的近似函数。

10. **SELU函数**（Scaled Exponential Linear Unit）：
    $$\text{SELU}(x) = \lambda \begin{cases} x & \text{if } x > 0 \\ \alpha(e^x - 1) & \text{otherwise} \end{cases}$$
- 具有自归一化的性质，在一定条件下可以保持网络的稳定性。

11. **Mish函数**：
    $$\text{Mish}(x) = x \cdot \tanh(\text{softplus}(x))$$
- 在一些情况下表现良好。

12. **ArcTan函数**（反正切函数）：
    $$\text{ArcTan}(x) = \arctan(x)$$
- 饱和性比较低，适合作为激活函数使用。

13. **SiLU函数**（Sigmoid Linear Unit）：
    $$\text{SiLU}(x) = x \cdot \text{sigmoid}(x)$$
- 也称作Sigmoid-Weighted Linear Unit，由Google提出。

14. **ISRU函数**（Inverse Square Root Unit）：
    $$\text{ISRU}(x, \alpha) = \frac{x}{\sqrt{1 + \alpha x^2}}$$
- 能够提升模型的泛化能力。
